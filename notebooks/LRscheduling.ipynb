{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget -c https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz -O 'imagenette2.tgz'\n",
        "!tar -xzf imagenette2.tgz\n",
        "!mv 'imagenette2-160' imagenette2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhQ2V4W8Gr_R",
        "outputId": "992c8512-4bd2-48b7-be73-907ebcb08aff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-13 21:33:22--  https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.133.37, 52.217.160.224, 52.216.229.93, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.133.37|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99003388 (94M) [application/x-tar]\n",
            "Saving to: ‘imagenette2.tgz’\n",
            "\n",
            "imagenette2.tgz     100%[===================>]  94.42M  27.3MB/s    in 3.5s    \n",
            "\n",
            "2023-03-13 21:33:25 (27.3 MB/s) - ‘imagenette2.tgz’ saved [99003388/99003388]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: The one change we need to make if we're in Colab is to uncomment this below block.\n",
        "# If we are in an ipython session or a notebook, clear the state to avoid bugs\n",
        "try:\n",
        "  _ = get_ipython().__class__.__name__\n",
        "  ## we set -f below to avoid prompting the user before clearing the notebook state\n",
        "  %reset -f\n",
        "except NameError:\n",
        "  pass ## we're still good"
      ],
      "metadata": {
        "id": "h8brFiCnYosI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "IMAGENETTE_CLASSES = {\n",
        "    'tench': 'n01440764',\n",
        "    'English springer': 'n02102040',\n",
        "    'cassette player': 'n02979186',\n",
        "    'chain saw': 'n03000684',\n",
        "    'church': 'n03028079',\n",
        "    'French horn': 'n03394916',\n",
        "    'garbage truck': 'n03417042',\n",
        "    'gas pump': 'n03425413',\n",
        "    'golf ball': 'n03445777',\n",
        "    'parachute': 'n03888257'\n",
        "}\n",
        "\n",
        "CLASSES_TO_IDX = {\n",
        "    'tench': 0,\n",
        "    'English springer': 1,\n",
        "    'cassette player': 2,\n",
        "    'chain saw': 3,\n",
        "    'church': 4,\n",
        "    'French horn': 5,\n",
        "    'garbage truck': 6,\n",
        "    'gas pump': 7,\n",
        "    'golf ball': 8,\n",
        "    'parachute': 9\n",
        "}\n",
        "\n",
        "class Imagenette(Dataset):\n",
        "\n",
        "    def __init__(self, path: str, train: bool):\n",
        "        self.path = path\n",
        "        self.train = train\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((64, 64)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "        if train:\n",
        "            self.path += '/train'\n",
        "        else:\n",
        "            self.path += '/val'\n",
        "\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        for label, folder in IMAGENETTE_CLASSES.items():\n",
        "            label_idx = CLASSES_TO_IDX[label]\n",
        "            for image in os.listdir(f'{self.path}/{folder}'):\n",
        "                self.images.append(f'{self.path}/{folder}/{image}')\n",
        "                self.labels.append(label_idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Load image with PIL\n",
        "        image = Image.open(self.images[index]).convert('RGB')\n",
        "        tensor = self.transform(image)\n",
        "        image.close()\n",
        "        return tensor, self.labels[index]\n",
        "\n",
        "dataset_train = Imagenette('imagenette2', train=True)\n",
        "print(len(dataset_train))\n",
        "print(dataset_train[0][0].shape, dataset_train[0][1])\n",
        "\n",
        "dataset_val = Imagenette('imagenette2', train=False)\n",
        "print(len(dataset_val))\n",
        "print(dataset_val[0][0].shape, dataset_val[0][1])"
      ],
      "metadata": {
        "id": "TW7I28UIS0Mr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59519099-8b03-4073-d54a-588f0132cef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9469\n",
            "torch.Size([3, 64, 64]) 0\n",
            "3925\n",
            "torch.Size([3, 64, 64]) 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: on CIFAR10, takes 50s for 10 epochs on Colab, for an accuracy of 94.21%"
      ],
      "metadata": {
        "id": "Bp3uF35UIOZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cell for testing\n",
        "\n",
        "import torch \n",
        "\n",
        "train_dataset_gpu_loader = torch.utils.data.DataLoader(dataset_train, batch_size=len(dataset_train), drop_last=True, shuffle=True)\n",
        "for item in next(iter(train_dataset_gpu_loader)):\n",
        "  print(\"ok\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3x8gx9rEaqzK",
        "outputId": "71a34433-909b-456c-be06-ca9b51df1348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ok\n",
            "ok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr_values = []\n",
        "\n",
        "\n",
        "########################################\n",
        "#           Train and Eval             #\n",
        "########################################\n",
        "\n",
        "def main():\n",
        "    # Initializing constants for the whole run.\n",
        "    net_ema = None ## Reset any existing network emas, we want to have _something_ to check for existence so we can initialize the EMA right from where the network is during training\n",
        "                   ## (as opposed to initializing the network_ema from the randomly-initialized starter network, then forcing it to play catch-up all of a sudden in the last several epochs)\n",
        "\n",
        "    total_time_seconds = 0.\n",
        "    current_steps = 0.\n",
        "\n",
        "    # TODO: Doesn't currently account for partial epochs really (since we're not doing \"real\" epochs across the whole batchsize)....\n",
        "    num_steps_per_epoch      = len(data['train']['images']) // batchsize\n",
        "    total_train_steps        = num_steps_per_epoch * hyp['misc']['train_epochs']\n",
        "    ema_epoch_start          = hyp['misc']['train_epochs'] - hyp['misc']['ema']['epochs']\n",
        "    num_cooldown_before_freeze_steps = 0\n",
        "    num_low_lr_steps_for_ema = hyp['misc']['ema']['epochs'] * num_steps_per_epoch\n",
        "\n",
        "    ## I believe this wasn't logged, but the EMA update power is adjusted by being raised to the power of the number of \"every n\" steps\n",
        "    ## to somewhat accomodate for whatever the expected information intake rate is. The tradeoff I believe, though, is that this is to some degree noisier as we\n",
        "    ## are intaking fewer samples of our distribution-over-time, with a higher individual weight each. This can be good or bad depending upon what we want.\n",
        "    projected_ema_decay_val  = hyp['misc']['ema']['decay_base'] ** hyp['misc']['ema']['every_n_steps']\n",
        "\n",
        "    # Adjust pct_start based upon how many epochs we need to finetune the ema at a low lr for\n",
        "    pct_start = hyp['opt']['percent_start'] #* (total_train_steps/(total_train_steps - num_low_lr_steps_for_ema))\n",
        "\n",
        "    # Get network\n",
        "    net = make_net()\n",
        "\n",
        "    ## Stowing the creation of these into a helper function to make things a bit more readable....\n",
        "    non_bias_params, bias_params = init_split_parameter_dictionaries(net)\n",
        "\n",
        "    # One optimizer for the regular network, and one for the biases. This allows us to use the superconvergence onecycle training policy for our networks....\n",
        "    opt = torch.optim.SGD(**non_bias_params)\n",
        "    opt_bias = torch.optim.SGD(**bias_params)\n",
        "\n",
        "    ## Not the most intuitive, but this basically takes us from ~0 to max_lr at the point pct_start, then down to .1 * max_lr at the end (since 1e16 * 1e-15 = .1 --\n",
        "    ##   This quirk is because the final lr value is calculated from the starting lr value and not from the maximum lr value set during training)\n",
        "    initial_div_factor = 1e16 # basically to make the initial lr ~0 or so :D\n",
        "\n",
        "    ########################################\n",
        "    #           Define LR Sched            #\n",
        "    ########################################\n",
        "    # lr_sched = torch.optim.lr_scheduler.OneCycleLR(opt,  max_lr=non_bias_params['lr'], pct_start=pct_start, div_factor=initial_div_factor, final_div_factor=1./(initial_div_factor*1e-24), total_steps=total_train_steps, anneal_strategy='linear', cycle_momentum=False)\n",
        "    # lr_sched = torch.optim.lr_scheduler.OneCycleLR(opt,  max_lr=non_bias_params['lr'], pct_start=pct_start, div_factor=initial_div_factor, final_div_factor=1./(initial_div_factor*1e-24), total_steps=total_train_steps, anneal_strategy='cos', cycle_momentum=False)\n",
        "    lr_sched = torch.optim.lr_scheduler.ExponentialLR(opt,gamma = 0.99)\n",
        "    # lr_sched      = torch.optim.lr_scheduler.CyclicLR(opt,base_lr = 0.0001, max_lr = 1e-3, step_size_up = 4, mode = \"triangular\")\n",
        "    # lr_sched      = torch.optim.lr_scheduler.CyclicLR(opt,base_lr = 0.0001, max_lr = 1e-3, step_size_up = 4, mode = \"triangular2\")\n",
        "    # lr_sched      = torch.optim.lr_scheduler.CyclicLR(opt,base_lr = 0.0001, max_lr = 1e-3, step_size_up = 4, mode = \"exp-range\")\n",
        "    \n",
        "    lr_sched_bias = torch.optim.lr_scheduler.OneCycleLR(opt_bias, max_lr=bias_params['lr'], pct_start=pct_start, div_factor=initial_div_factor, final_div_factor=1./(initial_div_factor*1e-24), total_steps=total_train_steps, anneal_strategy='linear', cycle_momentum=False)\n",
        "\n",
        "    ## For accurately timing GPU code\n",
        "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
        "    torch.cuda.synchronize() ## clean up any pre-net setup operations\n",
        "\n",
        "\n",
        "    # Get learning rates as each training step\n",
        "\n",
        "\n",
        "    if True: ## Sometimes we need a conditional/for loop here, this is placed to save the trouble of needing to indent\n",
        "        for epoch in range(hyp['misc']['train_epochs']):\n",
        "          #################\n",
        "          # Training Mode #\n",
        "          #################\n",
        "          torch.cuda.synchronize()\n",
        "          starter.record()\n",
        "          net.train()\n",
        "\n",
        "          loss_train = None\n",
        "          accuracy_train = None\n",
        "\n",
        "          for epoch_step, (inputs, targets) in enumerate(get_batches(data, key='train', batchsize=batchsize)):\n",
        "              ## Run everything through the network\n",
        "              outputs = net(inputs)\n",
        "              \n",
        "              loss_batchsize_scaler = 512/batchsize # to scale to keep things at a relatively similar amount of regularization when we change our batchsize since we're summing over the whole batch\n",
        "              ## If you want to add other losses or hack around with the loss, you can do that here.\n",
        "              loss = loss_fn(outputs, targets).mul(hyp['opt']['loss_scale_scaler']*loss_batchsize_scaler).sum().div(hyp['opt']['loss_scale_scaler']) ## Note, as noted in the original blog posts, the summing here does a kind of loss scaling\n",
        "                                                     ## (and is thus batchsize dependent as a result). This can be somewhat good or bad, depending...\n",
        "\n",
        "              # we only take the last-saved accs and losses from train\n",
        "              if epoch_step % 50 == 0:\n",
        "                  train_acc = (outputs.detach().argmax(-1) == targets).float().mean().item()\n",
        "                  train_loss = loss.detach().cpu().item()/(batchsize*loss_batchsize_scaler)\n",
        "\n",
        "              loss.backward()\n",
        "\n",
        "              ## Step for each optimizer, in turn.\n",
        "              opt.step()\n",
        "              opt_bias.step()\n",
        "\n",
        "              lr_values.append(opt.param_groups[0][\"lr\"])\n",
        "\n",
        "              # We only want to step the lr_schedulers while we have training steps to consume. Otherwise we get a not-so-friendly error from PyTorch\n",
        "              lr_sched.step()\n",
        "              lr_sched_bias.step()\n",
        "\n",
        "              ## Using 'set_to_none' I believe is slightly faster (albeit riskier w/ funky gradient update workflows) than under the default 'set to zero' method\n",
        "              opt.zero_grad(set_to_none=True)\n",
        "              opt_bias.zero_grad(set_to_none=True)\n",
        "              current_steps += 1\n",
        "\n",
        "              if epoch >= ema_epoch_start and current_steps % hyp['misc']['ema']['every_n_steps'] == 0:          \n",
        "                  ## Initialize the ema from the network at this point in time if it does not already exist.... :D\n",
        "                  if net_ema is None or epoch_step < num_cooldown_before_freeze_steps: # don't snapshot the network yet if so!\n",
        "                      net_ema = NetworkEMA(net, decay=projected_ema_decay_val)\n",
        "                      continue\n",
        "                  net_ema.update(net)\n",
        "          ender.record()\n",
        "          torch.cuda.synchronize()\n",
        "          total_time_seconds += 1e-3 * starter.elapsed_time(ender)\n",
        "\n",
        "\n",
        "\n",
        "          ####################\n",
        "          # Evaluation  Mode #\n",
        "          ####################\n",
        "          net.eval()\n",
        "\n",
        "          eval_batchsize = 785\n",
        "          assert data['eval']['images'].shape[0] % eval_batchsize == 0, \"Error: The eval batchsize must evenly divide the eval dataset (for now, we don't have drop_remainder implemented yet).\"\n",
        "          loss_list_val, acc_list, acc_list_ema = [], [], []\n",
        "          \n",
        "          with torch.no_grad():\n",
        "              for inputs, targets in get_batches(data, key='eval', batchsize=eval_batchsize):\n",
        "                  if epoch >= ema_epoch_start:\n",
        "                      outputs = net_ema(inputs)\n",
        "                      acc_list_ema.append((outputs.argmax(-1) == targets).float().mean())\n",
        "                  outputs = net(inputs)\n",
        "                  loss_list_val.append(loss_fn(outputs, targets).float().mean())\n",
        "                  acc_list.append((outputs.argmax(-1) == targets).float().mean())\n",
        "                  \n",
        "              val_acc = torch.stack(acc_list).mean().item()\n",
        "              ema_val_acc = None\n",
        "              # TODO: We can fuse these two operations (just above and below) all-together like :D :))))\n",
        "              if epoch >= ema_epoch_start:\n",
        "                  ema_val_acc = torch.stack(acc_list_ema).mean().item()\n",
        "\n",
        "              val_loss = torch.stack(loss_list_val).mean().item()\n",
        "          # We basically need to look up local variables by name so we can have the names, so we can pad to the proper column width.\n",
        "          ## Printing stuff in the terminal can get tricky and this used to use an outside library, but some of the required stuff seemed even\n",
        "          ## more heinous than this, unfortunately. So we switched to the \"more simple\" version of this!\n",
        "          format_for_table = lambda x, locals: (f\"{locals[x]}\".rjust(len(x))) \\\n",
        "                                                    if type(locals[x]) == int else \"{:0.4f}\".format(locals[x]).rjust(len(x)) \\\n",
        "                                                if locals[x] is not None \\\n",
        "                                                else \" \"*len(x)\n",
        "\n",
        "          # Print out our training details (sorry for the complexity, the whole logging business here is a bit of a hot mess once the columns need to be aligned and such....)\n",
        "          ## We also check to see if we're in our final epoch so we can print the 'bottom' of the table for each round.\n",
        "          print_training_details(list(map(partial(format_for_table, locals=locals()), logging_columns_list)), is_final_entry=(epoch == hyp['misc']['train_epochs'] - 1))\n",
        "    \n",
        "\n",
        "        # Visualize learinig rate scheduler\n",
        "        fig, ax = plt.subplots(1,1, figsize=(10,5))\n",
        "        ax.plot(range(len(lr_values)), lr_values,color='black',label=\"Learning Rate\")\n",
        "        ax.set_xlim([0, len(lr_values)])\n",
        "        ax.set_ylim([0,max(lr_values) + 0.0001])\n",
        "        ax.set_xlabel('Steps')\n",
        "        ax.set_ylabel('Learning Rate')\n",
        "        ax.spines['top'].set_visible(False)\n",
        "        ax.spines['right'].set_visible(False)\n",
        "        ax.set_title(label='ExponentialLR')\n",
        "        plt.show()\n",
        "    \n",
        "    return val_acc # Return the final non-ema accuracy achieved (not using the 'best accuracy' selection strategy, which I think is okay here....)\n",
        "                   # Note: For longer runs with much larer models, you may want to switch to the 'val_ema_acc' metric. This is because\n",
        "                   # that metric does much better outside of these extremely rapid training runs.\n",
        "\n",
        "print_training_details(logging_columns_list, column_heads_only=True) ## print out the training column heads before we print the actual content for each run.\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    acc_list = []\n",
        "    for run_num in range(1):\n",
        "        acc_list.append(torch.tensor(main()))\n",
        "    print(\"Mean and variance:\", (torch.mean(torch.stack(acc_list)).item(), torch.var(torch.stack(acc_list)).item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fg8t2puHLNO0",
        "outputId": "9d9fe993-9a3e-46d8-d739-af1ccf1a2074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------------------------------\n",
            "|  epoch  |  train_loss  |  val_loss  |  train_acc  |  val_acc  |  ema_val_acc  |  total_time_seconds  |\n",
            "--------------------------------------------------------------------------------------------------------\n",
            "|      0  |      2.3203  |    4.4679  |     0.0996  |   0.1159  |               |              0.9770  |\n",
            "|      1  |      3.5098  |    3.5604  |     0.1191  |   0.1192  |               |              1.9082  |\n",
            "|      2  |      2.3574  |    2.2868  |     0.1396  |   0.1781  |               |              2.8463  |\n",
            "|      3  |      2.2246  |    2.2001  |     0.2051  |   0.2410  |               |              3.7874  |\n",
            "|      4  |      2.1562  |    2.1294  |     0.2822  |   0.2718  |               |              4.7301  |\n",
            "|      5  |      2.1309  |    2.1440  |     0.2695  |   0.2673  |               |              5.6729  |\n",
            "|      6  |      2.0742  |    2.1424  |     0.3154  |   0.2145  |               |              6.6144  |\n",
            "|      7  |      2.0215  |    2.1330  |     0.3506  |   0.2915  |               |              7.5583  |\n",
            "|      8  |      1.9453  |    1.8850  |     0.3867  |   0.4257  |               |              8.5046  |\n",
            "|      9  |      1.8809  |    1.8732  |     0.4424  |   0.4148  |               |              9.4503  |\n",
            "|     10  |      1.8496  |    2.1682  |     0.4473  |   0.3465  |               |             10.4013  |\n",
            "|     11  |      1.8145  |    1.9939  |     0.4746  |   0.4079  |               |             11.3521  |\n",
            "|     12  |      1.8145  |    1.7744  |     0.4736  |   0.5083  |               |             12.3041  |\n",
            "|     13  |      1.7031  |    2.0570  |     0.5508  |   0.3893  |               |             13.2560  |\n",
            "|     14  |      1.7051  |    1.8922  |     0.5439  |   0.4262  |               |             14.2081  |\n",
            "|     15  |      1.6543  |    1.5575  |     0.5762  |   0.6311  |               |             15.1636  |\n",
            "|     16  |      1.6162  |    1.7380  |     0.6025  |   0.5470  |               |             16.1177  |\n",
            "|     17  |      1.5908  |    1.9152  |     0.6143  |   0.4558  |               |             17.0760  |\n",
            "|     18  |      1.5605  |    1.6462  |     0.6289  |   0.6331  |               |             18.0327  |\n",
            "|     19  |      1.5400  |    1.9184  |     0.6377  |   0.4566  |               |             18.9933  |\n",
            "|     20  |      1.4756  |    1.6043  |     0.6963  |   0.5967  |               |             19.9487  |\n",
            "|     21  |      1.4707  |    1.4763  |     0.6816  |   0.6882  |               |             20.9082  |\n",
            "|     22  |      1.4688  |    1.7250  |     0.6895  |   0.5361  |               |             21.8649  |\n",
            "|     23  |      1.4756  |    1.6776  |     0.6865  |   0.5595  |               |             22.8245  |\n",
            "|     24  |      1.4805  |    1.4993  |     0.7002  |   0.6594  |               |             23.7884  |\n",
            "|     25  |      1.4092  |    1.5779  |     0.7314  |   0.6268  |               |             24.7482  |\n",
            "|     26  |      1.3926  |    1.4145  |     0.7393  |   0.7358  |               |             25.7060  |\n",
            "|     27  |      1.3701  |    1.3581  |     0.7393  |   0.7702  |               |             26.6706  |\n",
            "|     28  |      1.3730  |    1.3612  |     0.7441  |   0.7674  |               |             27.6359  |\n",
            "|     29  |      1.3535  |    1.4046  |     0.7607  |   0.7254  |               |             28.5999  |\n",
            "|     30  |      1.3486  |    1.4449  |     0.7539  |   0.7032  |               |             29.5648  |\n",
            "|     31  |      1.3174  |    1.3545  |     0.7822  |   0.7781  |               |             30.5227  |\n",
            "|     32  |      1.3545  |    1.2879  |     0.7549  |   0.8102  |               |             31.4842  |\n",
            "|     33  |      1.3301  |    1.2854  |     0.7617  |   0.8143  |               |             32.4479  |\n",
            "|     34  |      1.3027  |    1.2938  |     0.7881  |   0.8054  |               |             33.4096  |\n",
            "|     35  |      1.2842  |    1.3223  |     0.8047  |   0.7936  |               |             34.3728  |\n",
            "|     36  |      1.3125  |    1.3295  |     0.7803  |   0.7868  |               |             35.3360  |\n",
            "|     37  |      1.3086  |    1.2548  |     0.7842  |   0.8326  |               |             36.3030  |\n",
            "|     38  |      1.2725  |    1.2574  |     0.7979  |   0.8380  |               |             37.2676  |\n",
            "|     39  |      1.2803  |    1.2420  |     0.7891  |   0.8354  |               |             38.2358  |\n",
            "|     40  |      1.2500  |    1.2569  |     0.8076  |   0.8357  |               |             39.2038  |\n",
            "|     41  |      1.2197  |    1.2314  |     0.8301  |   0.8415  |       0.8484  |             40.1680  |\n",
            "|     42  |      1.2285  |    1.2282  |     0.8203  |   0.8517  |       0.8558  |             41.1373  |\n",
            "|     43  |      1.2559  |    1.2413  |     0.8203  |   0.8425  |       0.8606  |             42.1035  |\n",
            "|     44  |      1.2227  |    1.2169  |     0.8350  |   0.8550  |       0.8647  |             43.0695  |\n",
            "|     45  |      1.2168  |    1.2285  |     0.8457  |   0.8482  |       0.8655  |             44.0351  |\n",
            "|     46  |      1.2070  |    1.2152  |     0.8379  |   0.8627  |       0.8680  |             45.0055  |\n",
            "|     47  |      1.1973  |    1.2041  |     0.8525  |   0.8668  |       0.8703  |             45.9748  |\n",
            "|     48  |      1.2090  |    1.2051  |     0.8281  |   0.8622  |       0.8698  |             46.9431  |\n",
            "|     49  |      1.2158  |    1.2123  |     0.8301  |   0.8591  |       0.8706  |             47.9098  |\n",
            "--------------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean and variance: (0.8591082692146301, nan)\n"
          ]
        }
      ]
    }
  ]
}