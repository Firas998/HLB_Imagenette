{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Adapting HyperLightspeedBench for the ImageNette dataset\n",
        "\n",
        "This Jupyter notebook combines the files `pytorch_imagenette.py` (dataset preparation) and `main.py` (the rest). You can load this notebook into Colab to reproduce the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhQ2V4W8Gr_R",
        "outputId": "9e0dc4cc-0b5d-4896-f30d-e0fdef85e867"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-02-19 20:52:24--  https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.45.246, 52.217.68.222, 54.231.203.224, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.45.246|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99003388 (94M) [application/x-tar]\n",
            "Saving to: ‘imagenette2.tgz’\n",
            "\n",
            "imagenette2.tgz     100%[===================>]  94.42M  27.1MB/s    in 3.5s    \n",
            "\n",
            "2023-02-19 20:52:28 (27.1 MB/s) - ‘imagenette2.tgz’ saved [99003388/99003388]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -c https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz -O 'imagenette2.tgz'\n",
        "!tar -xzf imagenette2.tgz\n",
        "!mv 'imagenette2-160' imagenette2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8brFiCnYosI"
      },
      "outputs": [],
      "source": [
        "# Note: The one change we need to make if we're in Colab is to uncomment this below block.\n",
        "# If we are in an ipython session or a notebook, clear the state to avoid bugs\n",
        "try:\n",
        "  _ = get_ipython().__class__.__name__\n",
        "  ## we set -f below to avoid prompting the user before clearing the notebook state\n",
        "  %reset -f\n",
        "except NameError:\n",
        "  pass ## we're still good"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TW7I28UIS0Mr",
        "outputId": "3823ca40-783e-43d4-fbc6-39eb52e22965"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9469\n",
            "torch.Size([3, 64, 64]) 0\n",
            "3925\n",
            "torch.Size([3, 64, 64]) 0\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "IMAGENETTE_CLASSES = {\n",
        "    'tench': 'n01440764',\n",
        "    'English springer': 'n02102040',\n",
        "    'cassette player': 'n02979186',\n",
        "    'chain saw': 'n03000684',\n",
        "    'church': 'n03028079',\n",
        "    'French horn': 'n03394916',\n",
        "    'garbage truck': 'n03417042',\n",
        "    'gas pump': 'n03425413',\n",
        "    'golf ball': 'n03445777',\n",
        "    'parachute': 'n03888257'\n",
        "}\n",
        "\n",
        "CLASSES_TO_IDX = {\n",
        "    'tench': 0,\n",
        "    'English springer': 1,\n",
        "    'cassette player': 2,\n",
        "    'chain saw': 3,\n",
        "    'church': 4,\n",
        "    'French horn': 5,\n",
        "    'garbage truck': 6,\n",
        "    'gas pump': 7,\n",
        "    'golf ball': 8,\n",
        "    'parachute': 9\n",
        "}\n",
        "\n",
        "class Imagenette(Dataset):\n",
        "\n",
        "    def __init__(self, path: str, train: bool):\n",
        "        self.path = path\n",
        "        self.train = train\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((64, 64)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "        if train:\n",
        "            self.path += '/train'\n",
        "        else:\n",
        "            self.path += '/val'\n",
        "\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        for label, folder in IMAGENETTE_CLASSES.items():\n",
        "            label_idx = CLASSES_TO_IDX[label]\n",
        "            for image in os.listdir(f'{self.path}/{folder}'):\n",
        "                self.images.append(f'{self.path}/{folder}/{image}')\n",
        "                self.labels.append(label_idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Load image with PIL\n",
        "        image = Image.open(self.images[index]).convert('RGB')\n",
        "        tensor = self.transform(image)\n",
        "        image.close()\n",
        "        return tensor, self.labels[index]\n",
        "\n",
        "dataset_train = Imagenette('imagenette2', train=True)\n",
        "print(len(dataset_train))\n",
        "print(dataset_train[0][0].shape, dataset_train[0][1])\n",
        "\n",
        "dataset_val = Imagenette('imagenette2', train=False)\n",
        "print(len(dataset_val))\n",
        "print(dataset_val[0][0].shape, dataset_val[0][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp3uF35UIOZZ"
      },
      "source": [
        "Note: on CIFAR10, takes 50s for 10 epochs on Colab, for an accuracy of 94.21%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3x8gx9rEaqzK",
        "outputId": "24de3965-82a8-4161-9488-5a297edb73c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ok\n",
            "ok\n"
          ]
        }
      ],
      "source": [
        "# cell for testing\n",
        "\n",
        "import torch \n",
        "\n",
        "train_dataset_gpu_loader = torch.utils.data.DataLoader(dataset_train, batch_size=len(dataset_train), drop_last=True, shuffle=True)\n",
        "for item in next(iter(train_dataset_gpu_loader)):\n",
        "  print(\"ok\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vak_TDrgEnVe",
        "outputId": "01dec182-8895-44af-8e8d-dbbfa63a191d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------------------------------\n",
            "|  epoch  |  train_loss  |  val_loss  |  train_acc  |  val_acc  |  ema_val_acc  |  total_time_seconds  |\n",
            "--------------------------------------------------------------------------------------------------------\n",
            "|      0  |      2.3164  |    2.2010  |     0.0830  |   0.2181  |               |              0.9869  |\n",
            "|      1  |      2.1875  |    3.5176  |     0.2607  |   0.2219  |               |              1.9681  |\n",
            "|      2  |      2.0000  |    1.9870  |     0.3633  |   0.4415  |               |              2.9532  |\n",
            "|      3  |      1.8555  |    1.8295  |     0.4658  |   0.5024  |               |              3.9377  |\n",
            "|      4  |      1.7832  |    1.8344  |     0.5117  |   0.5671  |               |              4.9288  |\n",
            "|      5  |      1.6904  |    2.3064  |     0.5527  |   0.3964  |               |              5.9238  |\n",
            "|      6  |      1.6787  |    2.1628  |     0.5605  |   0.4130  |               |              6.9187  |\n",
            "|      7  |      1.6865  |    1.8193  |     0.5625  |   0.5236  |               |              7.9194  |\n",
            "|      8  |      1.6064  |    1.7462  |     0.6162  |   0.5781  |               |              8.9203  |\n",
            "|      9  |      1.5186  |    1.6278  |     0.6611  |   0.6303  |               |              9.9263  |\n",
            "|     10  |      1.5264  |    1.8441  |     0.6357  |   0.5355  |               |             10.9330  |\n",
            "|     11  |      1.4766  |    1.5347  |     0.6777  |   0.6604  |               |             11.9483  |\n",
            "|     12  |      1.4727  |    1.7410  |     0.6875  |   0.5697  |               |             12.9634  |\n",
            "|     13  |      1.4648  |    1.9793  |     0.7012  |   0.4591  |               |             13.9813  |\n",
            "|     14  |      1.4512  |    1.7496  |     0.6973  |   0.5595  |               |             15.0033  |\n",
            "|     15  |      1.3945  |    1.4327  |     0.7275  |   0.7340  |               |             16.0272  |\n",
            "|     16  |      1.4355  |    2.2933  |     0.7109  |   0.3699  |               |             17.0537  |\n",
            "|     17  |      1.4424  |    1.5516  |     0.7061  |   0.6425  |               |             18.0832  |\n",
            "|     18  |      1.3633  |    1.5709  |     0.7500  |   0.6336  |               |             19.1156  |\n",
            "|     19  |      1.3682  |    1.4355  |     0.7461  |   0.7083  |               |             20.1495  |\n",
            "|     20  |      1.3359  |    1.5371  |     0.7695  |   0.6545  |               |             21.1781  |\n",
            "|     21  |      1.3770  |    1.7835  |     0.7451  |   0.5330  |               |             22.2046  |\n",
            "|     22  |      1.3291  |    1.5639  |     0.7715  |   0.6464  |               |             23.2306  |\n",
            "|     23  |      1.3359  |    1.4946  |     0.7686  |   0.6780  |               |             24.2569  |\n",
            "|     24  |      1.3320  |    1.4123  |     0.7734  |   0.7373  |               |             25.2777  |\n",
            "|     25  |      1.3213  |    1.2601  |     0.7676  |   0.8189  |               |             26.2909  |\n",
            "|     26  |      1.3213  |    1.4110  |     0.7803  |   0.7338  |               |             27.3076  |\n",
            "|     27  |      1.2891  |    1.5298  |     0.8018  |   0.6808  |               |             28.3201  |\n",
            "|     28  |      1.2852  |    1.2671  |     0.8008  |   0.8191  |               |             29.3307  |\n",
            "|     29  |      1.3037  |    1.3217  |     0.7920  |   0.7796  |               |             30.3378  |\n",
            "|     30  |      1.2617  |    1.4302  |     0.8154  |   0.7197  |               |             31.3457  |\n",
            "|     31  |      1.2842  |    1.2224  |     0.7949  |   0.8443  |               |             32.3530  |\n",
            "|     32  |      1.2090  |    1.5019  |     0.8467  |   0.6912  |               |             33.3568  |\n",
            "|     33  |      1.2725  |    1.2818  |     0.7939  |   0.8082  |               |             34.3605  |\n",
            "|     34  |      1.2314  |    1.2372  |     0.8330  |   0.8405  |               |             35.3640  |\n",
            "|     35  |      1.1963  |    1.2396  |     0.8379  |   0.8303  |               |             36.3701  |\n",
            "|     36  |      1.2021  |    1.1949  |     0.8486  |   0.8624  |               |             37.3747  |\n",
            "|     37  |      1.1963  |    1.1926  |     0.8506  |   0.8652  |               |             38.3794  |\n",
            "|     38  |      1.1885  |    1.2202  |     0.8457  |   0.8456  |               |             39.3834  |\n",
            "|     39  |      1.1758  |    1.1834  |     0.8545  |   0.8711  |               |             40.3895  |\n",
            "|     40  |      1.1523  |    1.1943  |     0.8730  |   0.8596  |               |             41.3931  |\n",
            "|     41  |      1.1553  |    1.2247  |     0.8682  |   0.8380  |       0.8104  |             42.3999  |\n",
            "|     42  |      1.1396  |    1.1855  |     0.8896  |   0.8708  |       0.8471  |             43.4090  |\n",
            "|     43  |      1.1260  |    1.1464  |     0.8994  |   0.8859  |       0.8683  |             44.4256  |\n",
            "|     44  |      1.1426  |    1.1592  |     0.8711  |   0.8815  |       0.8785  |             45.4402  |\n",
            "|     45  |      1.0742  |    1.1387  |     0.9209  |   0.8940  |       0.8833  |             46.4573  |\n",
            "|     46  |      1.0811  |    1.1332  |     0.9180  |   0.8971  |       0.8876  |             47.4783  |\n",
            "|     47  |      1.0879  |    1.1386  |     0.9189  |   0.8955  |       0.8940  |             48.4997  |\n",
            "|     48  |      1.0781  |    1.1295  |     0.9180  |   0.8991  |       0.8973  |             49.5192  |\n",
            "|     49  |      1.0479  |    1.1290  |     0.9316  |   0.9004  |       0.8994  |             50.5423  |\n",
            "--------------------------------------------------------------------------------------------------------\n",
            "Mean and variance: (0.9003822207450867, nan)\n"
          ]
        }
      ],
      "source": [
        "import functools\n",
        "from functools import partial\n",
        "import os\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "## <-- teaching comments\n",
        "# <-- functional comments\n",
        "# You can run 'sed -i.bak '/\\#\\#/d' ./main.py' to remove the teaching comments if they are in the way of your work. <3\n",
        "\n",
        "# This can go either way in terms of actually being helpful when it comes to execution speed.\n",
        "#torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# This code was built from the ground up to be directly hackable and to support rapid experimentation, which is something you might see\n",
        "# reflected in what would otherwise seem to be odd design decisions. It also means that maybe some cleaning up is required before moving\n",
        "# to production if you're going to use this code as such (such as breaking different section into unique files, etc). That said, if there's\n",
        "# ways this code could be improved and cleaned up, please do open a PR on the GitHub repo. Your support and help is much appreciated for this\n",
        "# project! :)\n",
        "\n",
        "\n",
        "# This is for testing that certain changes don't exceed some X% portion of the reference GPU (here an A100)\n",
        "# so we can help reduce a possibility that future releases don't take away the accessibility of this codebase.\n",
        "#torch.cuda.set_per_process_memory_fraction(fraction=6.5/40., device=0) ## 40. GB is the maximum memory of the base A100 GPU\n",
        "\n",
        "# set global defaults (in this particular file) for convolutions\n",
        "default_conv_kwargs = {'kernel_size': 3, 'padding': 'same', 'bias': False}\n",
        "\n",
        "batchsize = 1024\n",
        "bias_scaler = 32\n",
        "# To replicate the ~95.84%-accuracy-in-172-seconds runs, you can change the base_depth from 64->128, num_epochs from 10->90, ['ema'] epochs 9->78, and cutout 0->11\n",
        "hyp = {\n",
        "    'opt': {\n",
        "        'bias_lr':        2.1 * bias_scaler/512, # TODO: How we're expressing this information feels somewhat clunky, is there maybe a better way to do this? :'))))\n",
        "        'non_bias_lr':    2.1 / 512,\n",
        "        'bias_decay':     6.45e-4 * batchsize/bias_scaler,\n",
        "        'non_bias_decay': 6.45e-4 * batchsize,\n",
        "        'scaling_factor': 1./10,\n",
        "        'percent_start': .15,\n",
        "        'loss_scale_scaler': 4., # * Regularizer inside the loss summing (range: ~1/512 - 16+). FP8 should help with this somewhat too, whenever it comes out. :)\n",
        "    },\n",
        "    'net': {\n",
        "        'whitening': {\n",
        "            'kernel_size': 2,\n",
        "            'num_examples': 50000,\n",
        "        },\n",
        "        'batch_norm_momentum': .8, # Equivalent roughly to updating entirely every step, as momentum for batchnorm is represented in a different way (1 - momentum) due to a quirk of the original paper... ;'((((\n",
        "        'cutout_size': 0,\n",
        "        'pad_amount': 2,\n",
        "        'base_depth': 64 ## This should be a factor of 8 in some way to stay tensor core friendly\n",
        "    },\n",
        "    'misc': {\n",
        "        'ema': {\n",
        "            'epochs': 9,\n",
        "            'decay_base': .98,\n",
        "            'every_n_steps': 5,\n",
        "        },\n",
        "        'train_epochs': 50,\n",
        "        'device': 'cuda',\n",
        "        'data_location': 'data.pt',\n",
        "    }\n",
        "}\n",
        "\n",
        "#############################################\n",
        "#                Dataloader                 #\n",
        "#############################################\n",
        "\n",
        "if not os.path.exists(hyp['misc']['data_location']):\n",
        "\n",
        "        # use the dataloader to get a single batch of all of the dataset items at once.\n",
        "        train_dataset_gpu_loader = torch.utils.data.DataLoader(dataset_train, batch_size=len(dataset_train), drop_last=True,\n",
        "                                                  shuffle=True, num_workers=2, persistent_workers=False)\n",
        "        eval_dataset_gpu_loader = torch.utils.data.DataLoader(dataset_val, batch_size=len(dataset_val), drop_last=True,\n",
        "                                                  shuffle=False, num_workers=1, persistent_workers=False)\n",
        "\n",
        "        train_dataset_gpu = {}\n",
        "        eval_dataset_gpu = {}\n",
        "\n",
        "        train_dataset_gpu['images'], train_dataset_gpu['targets'] = [item.to(device=hyp['misc']['device'], non_blocking=True) for item in next(iter(train_dataset_gpu_loader))]\n",
        "        eval_dataset_gpu['images'],  eval_dataset_gpu['targets']  = [item.to(device=hyp['misc']['device'], non_blocking=True) for item in next(iter(eval_dataset_gpu_loader)) ]\n",
        "\n",
        "        cifar10_std, cifar10_mean = torch.std_mean(train_dataset_gpu['images'], dim=(0, 2, 3)) # dynamically calculate the std and mean from the data. this shortens the code and should help us adapt to new datasets!\n",
        "\n",
        "        def batch_normalize_images(input_images, mean, std):\n",
        "            return (input_images - mean.view(1, -1, 1, 1)) / std.view(1, -1, 1, 1)\n",
        "\n",
        "        # preload with our mean and std\n",
        "        batch_normalize_images = partial(batch_normalize_images, mean=cifar10_mean, std=cifar10_std)\n",
        "\n",
        "        ## Batch normalize datasets, now. Wowie. We did it! We should take a break and make some tea now.\n",
        "        train_dataset_gpu['images'] = batch_normalize_images(train_dataset_gpu['images'])\n",
        "        eval_dataset_gpu['images']  = batch_normalize_images(eval_dataset_gpu['images'])\n",
        "\n",
        "        data = {\n",
        "            'train': train_dataset_gpu,\n",
        "            'eval': eval_dataset_gpu\n",
        "        }\n",
        "\n",
        "        ## Convert dataset to FP16 now for the rest of the process....\n",
        "        data['train']['images'] = data['train']['images'].half()\n",
        "        data['eval']['images']  = data['eval']['images'].half()\n",
        "\n",
        "        torch.save(data, hyp['misc']['data_location'])\n",
        "\n",
        "else:\n",
        "    ## This is effectively instantaneous, and takes us practically straight to where the dataloader-loaded dataset would be. :)\n",
        "    ## So as long as you run the above loading process once, and keep the file on the disc it's specified by default in the above\n",
        "    ## hyp dictionary, then we should be good. :)\n",
        "    data = torch.load(hyp['misc']['data_location'])\n",
        "\n",
        "\n",
        "## As you'll note above and below, one difference is that we don't count loading the raw data to GPU since it's such a variable operation, and can sort of get in the way\n",
        "## of measuring other things. That said, measuring the preprocessing (outside of the padding) is still important to us.\n",
        "\n",
        "# Pad the GPU training dataset\n",
        "if hyp['net']['pad_amount'] > 0:\n",
        "    ## Uncomfortable shorthand, but basically we pad evenly on all _4_ sides with the pad_amount specified in the original dictionary\n",
        "    data['train']['images'] = F.pad(data['train']['images'], (hyp['net']['pad_amount'],)*4, 'reflect')\n",
        "\n",
        "#############################################\n",
        "#            Network Components             #\n",
        "#############################################\n",
        "\n",
        "# We might be able to fuse this weight and save some memory/runtime/etc, since the fast version of the network might be able to do without somehow....\n",
        "class BatchNorm(nn.BatchNorm2d):\n",
        "    def __init__(self, num_features, eps=1e-12, momentum=hyp['net']['batch_norm_momentum'], weight=False, bias=True):\n",
        "        super().__init__(num_features, eps=eps, momentum=momentum)\n",
        "        self.weight.data.fill_(1.0)\n",
        "        self.bias.data.fill_(0.0)\n",
        "        self.weight.requires_grad = weight\n",
        "        self.bias.requires_grad = bias\n",
        "\n",
        "# Allows us to set default arguments for the whole convolution itself.\n",
        "class Conv(nn.Conv2d):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        kwargs = {**default_conv_kwargs, **kwargs}\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.kwargs = kwargs\n",
        "\n",
        "# can hack any changes to each residual group that you want directly in here\n",
        "class ConvGroup(nn.Module):\n",
        "    def __init__(self, channels_in, channels_out, pool):\n",
        "        super().__init__()\n",
        "        self.pool = pool # todo: maybe we can condense this later\n",
        "\n",
        "        self.channels_in = channels_in\n",
        "        self.channels_out = channels_out\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv1 = Conv(channels_in, channels_out)\n",
        "        self.conv2 = Conv(channels_out, channels_out)\n",
        "        self.norm1 = BatchNorm(channels_out)\n",
        "        self.norm2 = BatchNorm(channels_out)\n",
        "        self.activ = nn.GELU()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        if self.pool:\n",
        "            x = self.pool1(x)\n",
        "        x = self.norm1(x)\n",
        "        x = self.activ(x)\n",
        "        residual = x\n",
        "        x = self.conv2(x)\n",
        "        x = self.norm2(x)\n",
        "        x = self.activ(x)\n",
        "        x = x + residual # haiku\n",
        "\n",
        "        return x\n",
        "\n",
        "class TemperatureScaler(nn.Module):\n",
        "    def __init__(self, init_val):\n",
        "        super().__init__()\n",
        "        self.scaler = torch.tensor(init_val)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x.float() ## save precision for the gradients in the backwards pass\n",
        "                  ## I personally believe from experience that this is important\n",
        "                  ## for a few reasons. I believe this is the main functional difference between\n",
        "                  ## my implementation, and David's implementation...\n",
        "        return x.mul(self.scaler)\n",
        "\n",
        "class FastGlobalMaxPooling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Previously was chained torch.max calls.\n",
        "        # requires less time than AdaptiveMax2dPooling -- about ~.3s for the entire run, in fact (which is pretty significant! :O :D :O :O <3 <3 <3 <3)\n",
        "        return torch.amax(x, dim=(2,3)) # Global maximum pooling\n",
        "\n",
        "#############################################\n",
        "#          Init Helper Functions            #\n",
        "#############################################\n",
        "\n",
        "def get_patches(x, patch_shape=(3, 3), dtype=torch.float32):\n",
        "    # This uses the unfold operation (https://pytorch.org/docs/stable/generated/torch.nn.functional.unfold.html?highlight=unfold#torch.nn.functional.unfold)\n",
        "    # to extract a _view_ (i.e., there's no data copied here) of blocks in the input tensor. We have to do it twice -- once horizontally, once vertically. Then\n",
        "    # from that, we get our kernel_size*kernel_size patches to later calculate the statistics for the whitening tensor on :D\n",
        "    c, (h, w) = x.shape[1], patch_shape\n",
        "    return x.unfold(2,h,1).unfold(3,w,1).transpose(1,3).reshape(-1,c,h,w).to(dtype) # TODO: Annotate?\n",
        "\n",
        "def get_whitening_parameters(patches):\n",
        "    # As a high-level summary, we're basically finding the high-dimensional oval that best fits the data here.\n",
        "    # We can then later use this information to map the input information to a nicely distributed sphere, where also\n",
        "    # the most significant features of the inputs each have their own axis. This significantly cleans things up for the\n",
        "    # rest of the neural network and speeds up training.\n",
        "    n,c,h,w = patches.shape\n",
        "    est_covariance = torch.cov(patches.view(n, c*h*w).t())\n",
        "    eigenvalues, eigenvectors = torch.linalg.eigh(est_covariance, UPLO='U') # this is the same as saying we want our eigenvectors, with the specification that the matrix be an upper triangular matrix (instead of a lower-triangular matrix)\n",
        "    return eigenvalues.flip(0).view(-1, 1, 1, 1), eigenvectors.t().reshape(c*h*w,c,h,w).flip(0)\n",
        "\n",
        "# Run this over the training set to calculate the patch statistics, then set the initial convolution as a non-learnable 'whitening' layer\n",
        "def init_whitening_conv(layer, train_set=None, num_examples=None, previous_block_data=None, pad_amount=None, freeze=True, whiten_splits=None):\n",
        "    if train_set is not None and previous_block_data is None:\n",
        "        if pad_amount > 0:\n",
        "            previous_block_data = train_set[:num_examples,:,pad_amount:-pad_amount,pad_amount:-pad_amount] # if it's none, we're at the beginning of our network.\n",
        "        else:\n",
        "            previous_block_data = train_set[:num_examples,:,:,:]\n",
        "\n",
        "    # chunking code to save memory for smaller-memory-size (generally consumer) GPUs\n",
        "    if whiten_splits is None:\n",
        "         previous_block_data_split = [previous_block_data] # If we're whitening in one go, then put it in a list for simplicity to reuse the logic below\n",
        "    else:\n",
        "         previous_block_data_split = previous_block_data.split(whiten_splits, dim=0) # Otherwise, we split this into different chunks to keep things manageable\n",
        "\n",
        "    eigenvalue_list, eigenvector_list = [], []\n",
        "    for data_split in previous_block_data_split:\n",
        "        eigenvalues, eigenvectors = get_whitening_parameters(get_patches(data_split, patch_shape=layer.weight.data.shape[2:]))\n",
        "        eigenvalue_list.append(eigenvalues)\n",
        "        eigenvector_list.append(eigenvectors)\n",
        "\n",
        "    eigenvalues = torch.stack(eigenvalue_list, dim=0).mean(0)\n",
        "    eigenvectors = torch.stack(eigenvector_list, dim=0).mean(0)\n",
        "    # i believe the eigenvalues and eigenvectors come out in float32 for this because we implicitly cast it to float32 in the patches function (for numerical stability)\n",
        "    set_whitening_conv(layer, eigenvalues.to(dtype=layer.weight.dtype), eigenvectors.to(dtype=layer.weight.dtype), freeze=freeze)\n",
        "    data = layer(previous_block_data.to(dtype=layer.weight.dtype))\n",
        "    return data\n",
        "\n",
        "def set_whitening_conv(conv_layer, eigenvalues, eigenvectors, eps=1e-2, freeze=True):\n",
        "    shape = conv_layer.weight.data.shape\n",
        "    conv_layer.weight.data[-eigenvectors.shape[0]:, :, :, :] = (eigenvectors/torch.sqrt(eigenvalues+eps))[-shape[0]:, :, :, :] # set the first n filters of the weight data to the top n significant (sorted by importance) filters from the eigenvectors\n",
        "    ## We don't want to train this, since this is implicitly whitening over the whole dataset\n",
        "    ## For more info, see David Page's original blogposts (link in the README.md as of this commit.)\n",
        "    if freeze: \n",
        "        conv_layer.weight.requires_grad = False\n",
        "\n",
        "\n",
        "#############################################\n",
        "#            Network Definition             #\n",
        "#############################################\n",
        "\n",
        "scaler = 2. ## You can play with this on your own if you want, for the first beta I wanted to keep things simple (for now) and leave it out of the hyperparams dict\n",
        "depths = {\n",
        "    'init':   round(scaler**-1*hyp['net']['base_depth']), # 32  w/ scaler at base value\n",
        "    'block1': round(scaler** 1*hyp['net']['base_depth']), # 128 w/ scaler at base value\n",
        "    'block2': round(scaler** 2*hyp['net']['base_depth']), # 256 w/ scaler at base value\n",
        "    'block3': round(scaler** 3*hyp['net']['base_depth']), # 512 w/ scaler at base value\n",
        "    'num_classes': 10\n",
        "}\n",
        "\n",
        "class SpeedyResNet(nn.Module):\n",
        "    def __init__(self, network_dict):\n",
        "        super().__init__()\n",
        "        self.net_dict = network_dict # flexible, defined in the make_net function\n",
        "\n",
        "    # This allows you to customize/change the execution order of the network as needed.\n",
        "    def forward(self, x):\n",
        "        if not self.training:\n",
        "            x = torch.cat((x, torch.flip(x, (-1,))))\n",
        "        x = self.net_dict['initial_block']['whiten'](x)\n",
        "        x = self.net_dict['initial_block']['project'](x)\n",
        "        x = self.net_dict['initial_block']['norm'](x)\n",
        "        x = self.net_dict['initial_block']['activation'](x)\n",
        "        x = self.net_dict['residual1'](x)\n",
        "        x = self.net_dict['residual2'](x)\n",
        "        x = self.net_dict['residual3'](x)\n",
        "        x = self.net_dict['pooling'](x)\n",
        "        x = self.net_dict['linear'](x)\n",
        "        x = self.net_dict['temperature'](x)\n",
        "        if not self.training:\n",
        "            # Average the predictions from the lr-flipped inputs during eval\n",
        "            orig, flipped = x.split(x.shape[0]//2, dim=0)\n",
        "            x = .5 * orig + .5 * flipped\n",
        "        return x\n",
        "\n",
        "def make_net():\n",
        "    # TODO: A way to make this cleaner??\n",
        "    # Note, you have to specify any arguments overlapping with defaults (i.e. everything but in/out depths) as kwargs so that they are properly overridden (TODO cleanup somehow?)\n",
        "    whiten_conv_depth = 3*hyp['net']['whitening']['kernel_size']**2\n",
        "    network_dict = nn.ModuleDict({\n",
        "        'initial_block': nn.ModuleDict({\n",
        "            'whiten': Conv(3, whiten_conv_depth, kernel_size=hyp['net']['whitening']['kernel_size'], padding=0),\n",
        "            'project': Conv(whiten_conv_depth, depths['init'], kernel_size=1),\n",
        "            'norm': BatchNorm(depths['init'], weight=False),\n",
        "            'activation': nn.GELU(),\n",
        "        }),\n",
        "        'residual1': ConvGroup(depths['init'],   depths['block1'], pool=True),\n",
        "        'residual2': ConvGroup(depths['block1'], depths['block2'], pool=True),\n",
        "        'residual3': ConvGroup(depths['block2'], depths['block3'], pool=True),\n",
        "        'pooling': FastGlobalMaxPooling(),\n",
        "        'linear': nn.Linear(depths['block3'], depths['num_classes'], bias=False),\n",
        "        'temperature': TemperatureScaler(hyp['opt']['scaling_factor'])\n",
        "    })\n",
        "\n",
        "    net = SpeedyResNet(network_dict)\n",
        "    net = net.to(hyp['misc']['device'])\n",
        "    net = net.to(memory_format=torch.channels_last) # to appropriately use tensor cores/avoid thrash while training\n",
        "    net.train()\n",
        "    net.half() # Convert network to half before initializing the initial whitening layer.\n",
        "\n",
        "    ## Initialize the whitening convolution\n",
        "    with torch.no_grad():\n",
        "        # Initialize the first layer to be fixed weights that whiten the expected input values of the network be on the unit hypersphere. (i.e. their...average vector length is 1.?, IIRC)\n",
        "        init_whitening_conv(net.net_dict['initial_block']['whiten'],\n",
        "                            data['train']['images'].index_select(0, torch.randperm(data['train']['images'].shape[0], device=data['train']['images'].device)),\n",
        "                            num_examples=hyp['net']['whitening']['num_examples'],\n",
        "                            pad_amount=hyp['net']['pad_amount'],\n",
        "                            whiten_splits=5000) ## Hardcoded for now while we figure out the optimal whitening number\n",
        "                                                ## If you're running out of memory (OOM) feel free to decrease this, but\n",
        "                                                ## the index lookup in the dataloader may give you some trouble depending\n",
        "                                                ## upon exactly how memory-limited you are\n",
        "\n",
        "    return net\n",
        "\n",
        "#############################################\n",
        "#            Data Preprocessing             #\n",
        "#############################################\n",
        "\n",
        "## This is actually (I believe) a pretty clean implementation of how to do something like this, since shifted-square masks unique to each depth-channel can actually be rather\n",
        "## tricky in practice. That said, if there's a better way, please do feel free to submit it! This can be one of the harder parts of the code to understand (though I personally get\n",
        "## stuck on the fold/unfold process for the lower-level convolution calculations.\n",
        "def make_random_square_masks(inputs, mask_size):\n",
        "    ##### TODO: Double check that this properly covers the whole range of values. :'( :')\n",
        "    if mask_size == 0:\n",
        "        return None # no need to cutout or do anything like that since the patch_size is set to 0\n",
        "    is_even = int(mask_size % 2 == 0)\n",
        "    in_shape = inputs.shape\n",
        "\n",
        "    # seed centers of squares to cutout boxes from, in one dimension each\n",
        "    mask_center_y = torch.empty(in_shape[0], dtype=torch.long, device=inputs.device).random_(mask_size//2-is_even, in_shape[-2]-mask_size//2-is_even)\n",
        "    mask_center_x = torch.empty(in_shape[0], dtype=torch.long, device=inputs.device).random_(mask_size//2-is_even, in_shape[-1]-mask_size//2-is_even)\n",
        "\n",
        "    # measure distance, using the center as a reference point\n",
        "    to_mask_y_dists = torch.arange(in_shape[-2], device=inputs.device).view(1, 1, in_shape[-2], 1) - mask_center_y.view(-1, 1, 1, 1)\n",
        "    to_mask_x_dists = torch.arange(in_shape[-1], device=inputs.device).view(1, 1, 1, in_shape[-1]) - mask_center_x.view(-1, 1, 1, 1)\n",
        "\n",
        "    to_mask_y = (to_mask_y_dists >= (-(mask_size // 2) + is_even)) * (to_mask_y_dists <= mask_size // 2)\n",
        "    to_mask_x = (to_mask_x_dists >= (-(mask_size // 2) + is_even)) * (to_mask_x_dists <= mask_size // 2)\n",
        "\n",
        "    final_mask = to_mask_y * to_mask_x ## Turn (y by 1) and (x by 1) boolean masks into (y by x) masks through multiplication. Their intersection is square, hurray! :D\n",
        "\n",
        "    return final_mask\n",
        "\n",
        "def batch_cutout(inputs, patch_size):\n",
        "    with torch.no_grad():\n",
        "        cutout_batch_mask = make_random_square_masks(inputs, patch_size)\n",
        "        if cutout_batch_mask is None:\n",
        "            return inputs # if the mask is None, then that's because the patch size was set to 0 and we will not be using cutout today.\n",
        "        # TODO: Could be fused with the crop operation for sheer speeeeeds. :D <3 :))))\n",
        "        cutout_batch = torch.where(cutout_batch_mask, torch.zeros_like(inputs), inputs)\n",
        "        return cutout_batch\n",
        "\n",
        "def batch_crop(inputs, crop_size):\n",
        "    with torch.no_grad():\n",
        "        crop_mask_batch = make_random_square_masks(inputs, crop_size)\n",
        "        cropped_batch = torch.masked_select(inputs, crop_mask_batch).view(inputs.shape[0], inputs.shape[1], crop_size, crop_size)\n",
        "        return cropped_batch\n",
        "\n",
        "def batch_flip_lr(batch_images, flip_chance=.5):\n",
        "    with torch.no_grad():\n",
        "        # TODO: Is there a more elegant way to do this? :') :'((((\n",
        "        return torch.where(torch.rand_like(batch_images[:, 0, 0, 0].view(-1, 1, 1, 1)) < flip_chance, torch.flip(batch_images, (-1,)), batch_images)\n",
        "\n",
        "\n",
        "########################################\n",
        "#          Training Helpers            #\n",
        "########################################\n",
        "\n",
        "class NetworkEMA(nn.Module):\n",
        "    def __init__(self, net, decay):\n",
        "        super().__init__() # init the parent module so this module is registered properly\n",
        "        self.net_ema = copy.deepcopy(net).eval().requires_grad_(False) # copy the model\n",
        "        self.decay = decay ## you can update/hack this as necessary for update scheduling purposes :3\n",
        "\n",
        "    def update(self, current_net):\n",
        "        with torch.no_grad():\n",
        "            for ema_net_parameter, (parameter_name, incoming_net_parameter) in zip(self.net_ema.state_dict().values(), current_net.state_dict().items()): # potential bug: assumes that the network architectures don't change during training (!!!!)\n",
        "                if incoming_net_parameter.dtype in (torch.half, torch.float):\n",
        "                    ema_net_parameter.mul_(self.decay).add_(incoming_net_parameter.detach().mul(1. - self.decay)) # update the ema values in place, similar to how optimizer momentum is coded\n",
        "                    if not 'running' in parameter_name:\n",
        "                        incoming_net_parameter = ema_net_parameter.detach()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        with torch.no_grad():\n",
        "            return self.net_ema(inputs)\n",
        "\n",
        "# TODO: Could we jit this in the (more distant) future? :)\n",
        "@torch.no_grad()\n",
        "def get_batches(data_dict, key, batchsize):\n",
        "    num_epoch_examples = len(data_dict[key]['images'])\n",
        "    shuffled = torch.randperm(num_epoch_examples, device='cuda')\n",
        "    crop_size = 32\n",
        "    ## Here, we prep the dataset by applying all data augmentations in batches ahead of time before each epoch, then we return an iterator below\n",
        "    ## that iterates in chunks over with a random derangement (i.e. shuffled indices) of the individual examples. So we get perfectly-shuffled\n",
        "    ## batches (which skip the last batch if it's not a full batch), but everything seems to be (and hopefully is! :D) properly shuffled. :)\n",
        "    if key == 'train':\n",
        "        images = batch_crop(data_dict[key]['images'], crop_size) # TODO: hardcoded image size for now?\n",
        "        images = batch_flip_lr(images)\n",
        "        images = batch_cutout(images, patch_size=hyp['net']['cutout_size'])\n",
        "    else:\n",
        "        images = data_dict[key]['images']\n",
        "\n",
        "    # Send the images to an (in beta) channels_last to help improve tensor core occupancy (and reduce NCHW <-> NHWC thrash) during training\n",
        "    images = images.to(memory_format=torch.channels_last)\n",
        "    for idx in range(num_epoch_examples // batchsize):\n",
        "        if not (idx+1)*batchsize > num_epoch_examples: ## Use the shuffled randperm to assemble individual items into a minibatch\n",
        "            yield images.index_select(0, shuffled[idx*batchsize:(idx+1)*batchsize]), \\\n",
        "                  data_dict[key]['targets'].index_select(0, shuffled[idx*batchsize:(idx+1)*batchsize]) ## Each item is only used/accessed by the network once per epoch. :D\n",
        "\n",
        "\n",
        "def init_split_parameter_dictionaries(network):\n",
        "    params_non_bias = {'params': [], 'lr': hyp['opt']['non_bias_lr'], 'momentum': .85, 'nesterov': True, 'weight_decay': hyp['opt']['non_bias_decay']}\n",
        "    params_bias     = {'params': [], 'lr': hyp['opt']['bias_lr'],     'momentum': .85, 'nesterov': True, 'weight_decay': hyp['opt']['bias_decay']}\n",
        "\n",
        "    for name, p in network.named_parameters():\n",
        "        if p.requires_grad:\n",
        "            if 'bias' in name:\n",
        "                params_bias['params'].append(p)\n",
        "            else:\n",
        "                params_non_bias['params'].append(p)\n",
        "    return params_non_bias, params_bias\n",
        "\n",
        "\n",
        "## Hey look, it's the soft-targets/label-smoothed loss! Native to PyTorch. Now, _that_ is pretty cool, and simplifies things a lot, to boot! :D :)\n",
        "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.2, reduction='none')\n",
        "\n",
        "logging_columns_list = ['epoch', 'train_loss', 'val_loss', 'train_acc', 'val_acc', 'ema_val_acc', 'total_time_seconds']\n",
        "# define the printing function and print the column heads\n",
        "def print_training_details(columns_list, separator_left='|  ', separator_right='  ', final=\"|\", column_heads_only=False, is_final_entry=False):\n",
        "    print_string = \"\"\n",
        "    if column_heads_only:\n",
        "        for column_head_name in columns_list:\n",
        "            print_string += separator_left + column_head_name + separator_right\n",
        "        print_string += final\n",
        "        print('-'*(len(print_string))) # print the top bar\n",
        "        print(print_string)\n",
        "        print('-'*(len(print_string))) # print the bottom bar\n",
        "    else:\n",
        "        for column_value in columns_list:\n",
        "            print_string += separator_left + column_value + separator_right\n",
        "        print_string += final\n",
        "        print(print_string)\n",
        "    if is_final_entry:\n",
        "        print('-'*(len(print_string))) # print the final output bar\n",
        "\n",
        "print_training_details(logging_columns_list, column_heads_only=True) ## print out the training column heads before we print the actual content for each run.\n",
        "\n",
        "########################################\n",
        "#           Train and Eval             #\n",
        "########################################\n",
        "\n",
        "def main():\n",
        "    # Initializing constants for the whole run.\n",
        "    net_ema = None ## Reset any existing network emas, we want to have _something_ to check for existence so we can initialize the EMA right from where the network is during training\n",
        "                   ## (as opposed to initializing the network_ema from the randomly-initialized starter network, then forcing it to play catch-up all of a sudden in the last several epochs)\n",
        "\n",
        "    total_time_seconds = 0.\n",
        "    current_steps = 0.\n",
        "\n",
        "    # TODO: Doesn't currently account for partial epochs really (since we're not doing \"real\" epochs across the whole batchsize)....\n",
        "    num_steps_per_epoch      = len(data['train']['images']) // batchsize\n",
        "    total_train_steps        = num_steps_per_epoch * hyp['misc']['train_epochs']\n",
        "    ema_epoch_start          = hyp['misc']['train_epochs'] - hyp['misc']['ema']['epochs']\n",
        "    num_cooldown_before_freeze_steps = 0\n",
        "    num_low_lr_steps_for_ema = hyp['misc']['ema']['epochs'] * num_steps_per_epoch\n",
        "\n",
        "    ## I believe this wasn't logged, but the EMA update power is adjusted by being raised to the power of the number of \"every n\" steps\n",
        "    ## to somewhat accomodate for whatever the expected information intake rate is. The tradeoff I believe, though, is that this is to some degree noisier as we\n",
        "    ## are intaking fewer samples of our distribution-over-time, with a higher individual weight each. This can be good or bad depending upon what we want.\n",
        "    projected_ema_decay_val  = hyp['misc']['ema']['decay_base'] ** hyp['misc']['ema']['every_n_steps']\n",
        "\n",
        "    # Adjust pct_start based upon how many epochs we need to finetune the ema at a low lr for\n",
        "    pct_start = hyp['opt']['percent_start'] #* (total_train_steps/(total_train_steps - num_low_lr_steps_for_ema))\n",
        "\n",
        "    # Get network\n",
        "    net = make_net()\n",
        "\n",
        "    ## Stowing the creation of these into a helper function to make things a bit more readable....\n",
        "    non_bias_params, bias_params = init_split_parameter_dictionaries(net)\n",
        "\n",
        "    # One optimizer for the regular network, and one for the biases. This allows us to use the superconvergence onecycle training policy for our networks....\n",
        "    opt = torch.optim.SGD(**non_bias_params)\n",
        "    opt_bias = torch.optim.SGD(**bias_params)\n",
        "\n",
        "    ## Not the most intuitive, but this basically takes us from ~0 to max_lr at the point pct_start, then down to .1 * max_lr at the end (since 1e16 * 1e-15 = .1 --\n",
        "    ##   This quirk is because the final lr value is calculated from the starting lr value and not from the maximum lr value set during training)\n",
        "    initial_div_factor = 1e16 # basically to make the initial lr ~0 or so :D\n",
        "    lr_sched      = torch.optim.lr_scheduler.OneCycleLR(opt,  max_lr=non_bias_params['lr'], pct_start=pct_start, div_factor=initial_div_factor, final_div_factor=1./(initial_div_factor*1e-24), total_steps=total_train_steps, anneal_strategy='linear', cycle_momentum=False)\n",
        "    lr_sched_bias = torch.optim.lr_scheduler.OneCycleLR(opt_bias, max_lr=bias_params['lr'], pct_start=pct_start, div_factor=initial_div_factor, final_div_factor=1./(initial_div_factor*1e-24), total_steps=total_train_steps, anneal_strategy='linear', cycle_momentum=False)\n",
        "\n",
        "    ## For accurately timing GPU code\n",
        "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
        "    torch.cuda.synchronize() ## clean up any pre-net setup operations\n",
        "\n",
        "\n",
        "    if True: ## Sometimes we need a conditional/for loop here, this is placed to save the trouble of needing to indent\n",
        "        for epoch in range(hyp['misc']['train_epochs']):\n",
        "          #################\n",
        "          # Training Mode #\n",
        "          #################\n",
        "          torch.cuda.synchronize()\n",
        "          starter.record()\n",
        "          net.train()\n",
        "\n",
        "          loss_train = None\n",
        "          accuracy_train = None\n",
        "\n",
        "          for epoch_step, (inputs, targets) in enumerate(get_batches(data, key='train', batchsize=batchsize)):\n",
        "              ## Run everything through the network\n",
        "              outputs = net(inputs)\n",
        "              \n",
        "              loss_batchsize_scaler = 512/batchsize # to scale to keep things at a relatively similar amount of regularization when we change our batchsize since we're summing over the whole batch\n",
        "              ## If you want to add other losses or hack around with the loss, you can do that here.\n",
        "              loss = loss_fn(outputs, targets).mul(hyp['opt']['loss_scale_scaler']*loss_batchsize_scaler).sum().div(hyp['opt']['loss_scale_scaler']) ## Note, as noted in the original blog posts, the summing here does a kind of loss scaling\n",
        "                                                     ## (and is thus batchsize dependent as a result). This can be somewhat good or bad, depending...\n",
        "\n",
        "              # we only take the last-saved accs and losses from train\n",
        "              if epoch_step % 50 == 0:\n",
        "                  train_acc = (outputs.detach().argmax(-1) == targets).float().mean().item()\n",
        "                  train_loss = loss.detach().cpu().item()/(batchsize*loss_batchsize_scaler)\n",
        "\n",
        "              loss.backward()\n",
        "\n",
        "              ## Step for each optimizer, in turn.\n",
        "              opt.step()\n",
        "              opt_bias.step()\n",
        "\n",
        "              # We only want to step the lr_schedulers while we have training steps to consume. Otherwise we get a not-so-friendly error from PyTorch\n",
        "              lr_sched.step()\n",
        "              lr_sched_bias.step()\n",
        "\n",
        "              ## Using 'set_to_none' I believe is slightly faster (albeit riskier w/ funky gradient update workflows) than under the default 'set to zero' method\n",
        "              opt.zero_grad(set_to_none=True)\n",
        "              opt_bias.zero_grad(set_to_none=True)\n",
        "              current_steps += 1\n",
        "\n",
        "              if epoch >= ema_epoch_start and current_steps % hyp['misc']['ema']['every_n_steps'] == 0:          \n",
        "                  ## Initialize the ema from the network at this point in time if it does not already exist.... :D\n",
        "                  if net_ema is None or epoch_step < num_cooldown_before_freeze_steps: # don't snapshot the network yet if so!\n",
        "                      net_ema = NetworkEMA(net, decay=projected_ema_decay_val)\n",
        "                      continue\n",
        "                  net_ema.update(net)\n",
        "          ender.record()\n",
        "          torch.cuda.synchronize()\n",
        "          total_time_seconds += 1e-3 * starter.elapsed_time(ender)\n",
        "\n",
        "          ####################\n",
        "          # Evaluation  Mode #\n",
        "          ####################\n",
        "          net.eval()\n",
        "\n",
        "          eval_batchsize = 785\n",
        "          assert data['eval']['images'].shape[0] % eval_batchsize == 0, \"Error: The eval batchsize must evenly divide the eval dataset (for now, we don't have drop_remainder implemented yet).\"\n",
        "          loss_list_val, acc_list, acc_list_ema = [], [], []\n",
        "          \n",
        "          with torch.no_grad():\n",
        "              for inputs, targets in get_batches(data, key='eval', batchsize=eval_batchsize):\n",
        "                  if epoch >= ema_epoch_start:\n",
        "                      outputs = net_ema(inputs)\n",
        "                      acc_list_ema.append((outputs.argmax(-1) == targets).float().mean())\n",
        "                  outputs = net(inputs)\n",
        "                  loss_list_val.append(loss_fn(outputs, targets).float().mean())\n",
        "                  acc_list.append((outputs.argmax(-1) == targets).float().mean())\n",
        "                  \n",
        "              val_acc = torch.stack(acc_list).mean().item()\n",
        "              ema_val_acc = None\n",
        "              # TODO: We can fuse these two operations (just above and below) all-together like :D :))))\n",
        "              if epoch >= ema_epoch_start:\n",
        "                  ema_val_acc = torch.stack(acc_list_ema).mean().item()\n",
        "\n",
        "              val_loss = torch.stack(loss_list_val).mean().item()\n",
        "          # We basically need to look up local variables by name so we can have the names, so we can pad to the proper column width.\n",
        "          ## Printing stuff in the terminal can get tricky and this used to use an outside library, but some of the required stuff seemed even\n",
        "          ## more heinous than this, unfortunately. So we switched to the \"more simple\" version of this!\n",
        "          format_for_table = lambda x, locals: (f\"{locals[x]}\".rjust(len(x))) \\\n",
        "                                                    if type(locals[x]) == int else \"{:0.4f}\".format(locals[x]).rjust(len(x)) \\\n",
        "                                                if locals[x] is not None \\\n",
        "                                                else \" \"*len(x)\n",
        "\n",
        "          # Print out our training details (sorry for the complexity, the whole logging business here is a bit of a hot mess once the columns need to be aligned and such....)\n",
        "          ## We also check to see if we're in our final epoch so we can print the 'bottom' of the table for each round.\n",
        "          print_training_details(list(map(partial(format_for_table, locals=locals()), logging_columns_list)), is_final_entry=(epoch == hyp['misc']['train_epochs'] - 1))\n",
        "    return val_acc # Return the final non-ema accuracy achieved (not using the 'best accuracy' selection strategy, which I think is okay here....)\n",
        "                   # Note: For longer runs with much larer models, you may want to switch to the 'val_ema_acc' metric. This is because\n",
        "                   # that metric does much better outside of these extremely rapid training runs.\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    acc_list = []\n",
        "    for run_num in range(1):\n",
        "        acc_list.append(torch.tensor(main()))\n",
        "    print(\"Mean and variance:\", (torch.mean(torch.stack(acc_list)).item(), torch.var(torch.stack(acc_list)).item()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
